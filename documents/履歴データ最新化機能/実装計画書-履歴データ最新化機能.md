詳細設計書に基づき、**実装計画書（Implementation Plan）** を作成します。

本計画書では、`RakutenData` への蓄積・保存ロジックの実装と、それを `RealtimeTrader` の停止時に呼び出すフローを具体化します。また、前回の「再起動方式（スーパーバイザー化）」の改修と合わせて、`run_realtrade.py` を統合的に更新します。

-----

# 実装計画書: 履歴データ最新化機能 (Historical Data Updater)

**プロジェクト:** StockAutoV3
**作成日:** 2025/11/25
**バージョン:** 1.0
**実装担当:** AIアシスタント & ユーザー
**対象ファイル:**

1.  `src/realtrade/rakuten/rakuten_data.py`
2.  `src/realtrade/cerebro_factory.py`
3.  `src/realtrade/run_realtrade.py`

## 1\. 概要

システム停止時（15:30）に、当日メモリ上に生成された確定足をCSVファイルに追記保存する機能を実装する。これにより、翌朝の再起動時にインジケーターが正しく計算（ウォームアップ）される状態を保証する。

## 2\. 実装ステップ

### ステップ 1: `RakutenData` クラスの改修

`src/realtrade/rakuten/rakuten_data.py` を修正する。

1.  **`__init__`**:
      * 引数 `save_file` を追加し、保存。
      * データ蓄積用リスト `self._new_bars = []` を初期化。
2.  **`_load`**:
      * `self.builder.add_tick()` が完了したバーを返した際、そのコピーを `self._new_bars` に `append` する処理を追加。
3.  **`save_history` (新規)**:
      * `pandas` を使用してメモリ上のデータと既存CSVをマージするロジックを実装。
      * `concat` -\> `drop_duplicates` -\> `sort_values` -\> `to_csv` の流れを実装。

### ステップ 2: `CerebroFactory` の改修

`src/realtrade/cerebro_factory.py` を修正する。

1.  **`create_instance`**:
      * 履歴データ読み込み時に使用したファイルパス（`latest_file`）を特定する。
      * ファイルがない場合は、新規ファイルパス（`{symbol}_{compression}m_{year}.csv`）を生成する。
      * `RakutenData` の初期化引数 `save_file` にそのパスを渡す。

### ステップ 3: `RealtimeTrader` クラスの改修 (`run_realtrade.py`)

`src/realtrade/run_realtrade.py` を修正する。
※ 前回の「再起動方式」による `main` ループの変更と統合して実装する。

1.  **`RealtimeTrader.stop`**:
      * `cerebro.datas` をループし、`save_history` メソッドを持つオブジェクトがあれば実行するロジックを追加。
      * この処理を `connector.stop()` などのリソース開放前に行う。

## 3\. 生成されるコード (`tmp.py` 用)

以下のコードは、前回の「再起動方式」の改修内容を含んだ統合版として生成される。

### 3.1. `src/realtrade/rakuten/rakuten_data.py`

```python
import backtrader as bt
from datetime import datetime, timedelta, time
import logging
import pandas as pd
import threading
import os

from ..bar_builder import BarBuilder

logger = logging.getLogger(__name__)

class RakutenData(bt.feeds.PandasData):
    
    params = (
        ('bridge', None),
        ('symbol', None),
        ('timeframe', bt.TimeFrame.Minutes),
        ('heartbeat', 1.0),
        ('save_file', None), # 新規: 保存先ファイルパス
    )

    def __init__(self):
        self._hist_df = self.p.dataname
        
        empty_df = pd.DataFrame(
            columns=['datetime', 'open', 'high', 'low', 'close', 'volume', 'openinterest']
        )
        empty_df = empty_df.set_index('datetime')
        self.p.dataname = empty_df
        
        super(RakutenData, self).__init__()
        
        if self.p.bridge is None:
            raise ValueError("ExcelBridgeインスタンスが 'bridge' パラメータとして渡されていません。")
        if self.p.symbol is None:
            raise ValueError("銘柄コードが 'symbol' パラメータとして渡されていません。")
            
        self.bridge = self.p.bridge
        self.symbol = str(self.p.symbol)
        
        self.last_dt = None
        self._stopevent = threading.Event()
        
        self.builder = BarBuilder(interval_minutes=self.p.compression)

        # ▼▼▼ 履歴データ保存用 ▼▼▼
        self.save_file = self.p.save_file
        self._new_bars = [] # 当日の確定足を蓄積するバッファ
        # ▲▲▲ 履歴データ保存用 ▲▲▲

        self.history_supplied = False if (self._hist_df is not None and not self._hist_df.empty) else True

    def stop(self):
        self._stopevent.set()

    def save_history(self):
        """蓄積された当日の確定足をCSVに保存・マージする"""
        if not self._new_bars:
            logger.info(f"[{self.symbol}] 保存すべき新規データはありません。")
            return

        if not self.save_file:
            logger.warning(f"[{self.symbol}] 保存先ファイルが指定されていないため、履歴保存をスキップします。")
            return

        try:
            # 1. 新規データをDataFrame化
            new_df = pd.DataFrame(self._new_bars)
            # カラム名をBacktrader標準形式に合わせる（BarBuilderのキー依存）
            # BarBuilderは 'timestamp', 'open', 'high', 'low', 'close', 'volume' を返す
            new_df.rename(columns={'timestamp': 'datetime'}, inplace=True)
            if 'openinterest' not in new_df.columns:
                new_df['openinterest'] = 0

            # 2. 既存データとのマージ
            if os.path.exists(self.save_file):
                try:
                    old_df = pd.read_csv(self.save_file, parse_dates=['datetime'])
                    # 念のためカラム名を小文字化
                    old_df.columns = [c.lower() for c in old_df.columns]
                    merged_df = pd.concat([old_df, new_df])
                except Exception as e:
                    logger.error(f"[{self.symbol}] 既存CSVの読み込みに失敗しました。新規作成します: {e}")
                    merged_df = new_df
            else:
                merged_df = new_df

            # 3. 重複排除とソート
            # datetimeを基準に重複を排除（最新を優先）
            merged_df.drop_duplicates(subset=['datetime'], keep='last', inplace=True)
            merged_df.sort_values(by='datetime', inplace=True)

            # 4. 保存
            # ディレクトリがない場合は作成
            os.makedirs(os.path.dirname(self.save_file), exist_ok=True)
            merged_df.to_csv(self.save_file, index=False)
            
            logger.info(f"[{self.symbol}] 履歴データを保存しました: {self.save_file} (+{len(self._new_bars)} records)")
            
            # バッファをクリア
            self._new_bars = []

        except Exception as e:
            logger.error(f"[{self.symbol}] 履歴データの保存中にエラーが発生しました: {e}", exc_info=True)

    def _load(self):
        if self._hist_df is not None and not self._hist_df.empty:
            row = self._hist_df.iloc[0]
            self._hist_df = self._hist_df.iloc[1:]
            self._populate_lines_from_series(row)
            logger.debug(f"[{self.symbol}] 過去データを供給: {row.name}")
            if self._hist_df.empty:
                self.history_supplied = True
            return True

        if self._stopevent.is_set():
            return False

        current_dt = datetime.now()
        if self.last_dt and (current_dt - self.last_dt) < timedelta(seconds=self.p.heartbeat):
            return None
        
        latest_data = self.bridge.get_latest_data(self.symbol)

        if not latest_data or latest_data.get('close') is None:
            return self._load_heartbeat()
        
        current_time = current_dt.time()
        is_morning = time(9, 0) <= current_time <= time(11, 30)
        is_afternoon = time(12, 30) <= current_time <= time(15, 30)

        if not (is_morning or is_afternoon):
            # 取引時間外はTickを無視（スーパーバイザー側でプロセス停止される想定だが、安全のため）
            return None

        price = latest_data['close']
        volume = latest_data.get('volume', 0.0)
        completed_bar = self.builder.add_tick(current_dt, price, volume)

        if completed_bar:
            logger.info(f"[{self.symbol}] 新規5分足完成: {completed_bar['timestamp']}")
            # ▼▼▼ 蓄積 ▼▼▼
            self._new_bars.append(completed_bar.copy()) 
            # ▲▲▲ 蓄積 ▲▲▲
            self._populate_lines_from_dict(completed_bar)
            return True
        else:
            return None

    def _load_heartbeat(self):
        if len(self.lines.close) == 0 or self.lines.close[0] is None:
             return None
        last_close = self.lines.close[0]
        epsilon = 0.0 if last_close is None else last_close * 0.0001
        current_dt = datetime.now()
        row = {
            'timestamp': current_dt, 'open': last_close, 'high': last_close + epsilon,
            'low': last_close, 'close': last_close, 'volume': 0, 'openinterest': 0
        }
        self._populate_lines_from_dict(row, is_heartbeat=True)
        return True
        
    def _populate_lines_from_dict(self, bar_dict: dict, is_heartbeat: bool = False):
        dt = bar_dict['timestamp']
        self.lines.datetime[0] = self.date2num(dt)
        self.lines.open[0] = float(bar_dict['open'])
        self.lines.high[0] = float(bar_dict['high'])
        self.lines.low[0] = float(bar_dict['low'])
        self.lines.close[0] = float(bar_dict['close'])
        self.lines.volume[0] = float(bar_dict.get('volume', 0))
        self.lines.openinterest[0] = float(bar_dict.get('openinterest', 0))
        if not is_heartbeat:
            self.last_dt = dt

    def _populate_lines_from_series(self, bar_series: pd.Series):
        dt = pd.to_datetime(bar_series.name).to_pydatetime()
        self.lines.datetime[0] = self.date2num(dt)
        self.lines.open[0] = float(bar_series['open'])
        self.lines.high[0] = float(bar_series['high'])
        self.lines.low[0] = float(bar_series['low'])
        self.lines.close[0] = float(bar_series['close'])
        self.lines.volume[0] = float(bar_series.get('volume', 0))
        self.lines.openinterest[0] = float(bar_series.get('openinterest', 0))
        self.last_dt = dt
        
    def flush(self):
        # スーパーバイザー方式に移行するため、flushの重要性は下がるが、
        # 強制終了時に念のため最後の作りかけバーを保存するかは要検討。
        # 現状は確定足のみ保存の方針とするため、flushされたバーも保存対象とする。
        final_bar = self.builder.flush()
        if final_bar:
            self._new_bars.append(final_bar.copy()) # 保存対象に追加
            self._populate_lines_from_dict(final_bar)
            logger.info(f"[{self.symbol}] 最終バーをフラッシュ供給: {final_bar['timestamp']}")
```

### 3.2. `src/realtrade/cerebro_factory.py`

```python
import backtrader as bt
import copy
import yaml
import logging
import os
import glob
import pandas as pd
from datetime import datetime # 追加

from .strategy import RealTradeStrategy
from .rakuten.rakuten_broker import RakutenBroker
from .rakuten.rakuten_data import RakutenData

logger = logging.getLogger(__name__)

class CerebroFactory:
    def __init__(self, strategy_catalog, base_strategy_params, data_dir, statistics_map):
        self.strategy_catalog = strategy_catalog
        self.base_strategy_params = base_strategy_params
        self.data_dir = data_dir
        self.statistics_map = statistics_map
        logger.info("CerebroFactory initialized.")

    def create_instance(self, symbol: str, strategy_name: str, connector):
        entry_strategy_def = next((item for item in self.strategy_catalog if item["name"] == strategy_name), None)
        if not entry_strategy_def:
            logger.warning(f"Strategy definition not found for '{strategy_name}'. Skipping symbol {symbol}.")
            return None

        strategy_params = copy.deepcopy(self.base_strategy_params)
        strategy_params.update(entry_strategy_def)
        strategy_params['strategy_name'] = strategy_name

        try:
            cerebro = bt.Cerebro(runonce=False)
            cerebro.setbroker(RakutenBroker(bridge=connector))
            
            short_tf_config = strategy_params['timeframes']['short']
            compression = short_tf_config['compression']
            search_pattern = os.path.join(self.data_dir, f"{symbol}_{compression}m_*.csv")
            files = glob.glob(search_pattern)
            
            hist_df = pd.DataFrame()
            save_file_path = None # 保存先パス

            if files:
                latest_file = max(files, key=os.path.getctime)
                save_file_path = latest_file # 既存ファイルがあればそれを保存先にする
                try:
                    df = pd.read_csv(latest_file, index_col='datetime', parse_dates=True)
                    if not df.empty:
                        if df.index.tz is not None: df.index = df.index.tz_localize(None)
                        df.columns = [x.lower() for x in df.columns]
                        hist_df = df
                        logger.info(f"[{symbol}] Loaded {len(hist_df)} bars from {latest_file}")
                except Exception as e:
                    logger.warning(f"[{symbol}] Could not load historical data from {latest_file}: {e}")
            
            # 既存ファイルがない場合は、新規ファイルパスを生成
            if not save_file_path:
                year = datetime.now().year
                save_file_path = os.path.join(self.data_dir, f"{symbol}_{compression}m_{year}.csv")
                logger.info(f"[{symbol}] 新規保存先を設定: {save_file_path}")

            primary_data = RakutenData(
                dataname=hist_df,
                bridge=connector,
                symbol=symbol,
                timeframe=bt.TimeFrame.TFrame(short_tf_config['timeframe']),
                compression=short_tf_config['compression'],
                save_file=save_file_path # ▼▼▼ 保存先を渡す ▼▼▼
            )
            cerebro.adddata(primary_data, name=str(symbol))

            for tf_name in ['medium', 'long']:
                if tf_config := strategy_params['timeframes'].get(tf_name):
                    cerebro.resampledata(
                        primary_data,
                        timeframe=bt.TimeFrame.TFrame(tf_config['timeframe']),
                        compression=tf_config['compression'],
                        name=tf_name
                    )
            
            stats_key = (strategy_name, str(symbol))
            symbol_statistics = self.statistics_map.get(stats_key, {})
            
            strategy_components = {
                'statistics': symbol_statistics
            }
            
            cerebro.addstrategy(
                RealTradeStrategy, 
                strategy_params=strategy_params, 
                strategy_components=strategy_components
            )
            
            return cerebro

        except Exception as e:
            logger.error(f"[{symbol}] Failed to create Cerebro instance: {e}", exc_info=True)
            return None
```

### 3.3. `src/realtrade/run_realtrade.py`

※ 「再起動方式（スーパーバイザー化）」と「履歴保存呼び出し」の両方を含んだコード。

```python
import logging
import time as time_module
import yaml
import pandas as pd
import glob
import os
import sys
import threading
from datetime import datetime, time, timedelta

# --- Project Root Setup ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

# --- Local Imports ---
from src.core.util import logger as logger_setup, notifier
from . import config_realtrade as config
from .bridge.excel_connector import ExcelConnector
from .position_synchronizer import PositionSynchronizer
from .cerebro_factory import CerebroFactory

logger = logging.getLogger(__name__)

class RealtimeTrader:
    def __init__(self):
        self.strategy_catalog = self._load_yaml(os.path.join(config.BASE_DIR, 'config', 'strategy_catalog.yml'))
        self.base_strategy_params = self._load_yaml(os.path.join(config.BASE_DIR, 'config', 'strategy_base.yml'))
        self.trade_data = self._load_trade_data(config.RECOMMEND_FILE_PATTERN)
        
        self.strategy_assignments = pd.Series(
            self.trade_data['戦略名'].values, 
            index=self.trade_data['銘柄'].astype(str)
        ).to_dict()
        
        self.statistics_map = {}
        cols_to_load = ['Kelly_Adj', 'Kelly_Raw']
        for _, row in self.trade_data.iterrows():
            key = (row['戦略名'], str(row['銘柄']))
            stats = {col: row[col] for col in cols_to_load if col in row}
            self.statistics_map[key] = stats

        self.symbols = list(self.strategy_assignments.keys())
        
        self.threads = []
        self.cerebro_instances = []
        self.strategy_instances = {}
        self.stop_event = threading.Event()
        
        self.connector = ExcelConnector(workbook_path=config.EXCEL_WORKBOOK_PATH)
        
        self.factory = CerebroFactory(
            self.strategy_catalog, 
            self.base_strategy_params, 
            config.DATA_DIR,
            self.statistics_map
        )
        self.synchronizer = PositionSynchronizer(self.connector, self.strategy_instances, self.stop_event)

    def _load_yaml(self, fp):
        with open(fp, 'r', encoding='utf-8') as f: return yaml.safe_load(f)
        
    def _load_trade_data(self, pattern):
        files = glob.glob(pattern)
        if not files: raise FileNotFoundError(f"Recommendation file not found: {pattern}")
        latest_file = max(files, key=os.path.getctime)
        logger.info(f"Loading recommended strategies and stats from: {latest_file}")
        df = pd.read_csv(latest_file)
        if 'Kelly_Adj' not in df.columns or 'Kelly_Raw' not in df.columns:
            logger.warning(f"警告: {latest_file} に 'Kelly_Adj' または 'Kelly_Raw' が見つかりません。")
        return df

    def _run_cerebro(self, cerebro_instance):
        try:
            cerebro_instance.run()
        except Exception as e:
            logger.error(f"Cerebro thread crashed: {e}", exc_info=True)
        logger.info(f"Cerebro thread finished: {threading.current_thread().name}")

    def start(self):
        logger.info("Starting RealtimeTrader components...")
        self.connector.start()

        for symbol in self.symbols:
            strategy_name = self.strategy_assignments.get(str(symbol))
            if not strategy_name:
                logger.warning(f"No strategy assigned for symbol {symbol}. Skipping.")
                continue
            
            cerebro = self.factory.create_instance(symbol, strategy_name, self.connector)
            
            if cerebro:
                self.cerebro_instances.append(cerebro)
                self.strategy_instances[str(symbol)] = cerebro.strats[0][0]
                
                t = threading.Thread(target=self._run_cerebro, args=(cerebro,), name=f"Cerebro-{symbol}", daemon=True)
                self.threads.append(t)
                t.start()
        
        self.synchronizer.start()
        logger.info("RealtimeTrader started successfully.")

    def stop(self):
        """全コンポーネントを安全に停止し、データを保存する。"""
        logger.info("Stopping RealtimeTrader...")
        self.stop_event.set()

        # 1. データの保存 (新規追加)
        logger.info("Saving history data for all feeds...")
        for cerebro in self.cerebro_instances:
            if cerebro.datas:
                # Primary data feed (RakutenData) は通常 datas[0]
                data_feed = cerebro.datas[0]
                if hasattr(data_feed, 'save_history'):
                    # 念のため flush してから保存
                    if hasattr(data_feed, 'flush'):
                        data_feed.flush()
                    data_feed.save_history()

        # 2. データフィードの停止
        for cerebro in self.cerebro_instances:
            if cerebro.datas and hasattr(cerebro.datas[0], 'stop'):
                cerebro.datas[0].stop()

        # 3. スレッドの終了待機
        if self.synchronizer.is_alive():
            self.synchronizer.join(timeout=5)
        for t in self.threads:
            if t.is_alive():
                t.join(timeout=5)
        
        # 4. Excelコネクタの停止
        self.connector.stop()
        logger.info("RealtimeTrader stopped.")

# --- Supervisor Functions ---

def is_market_active(now: datetime) -> bool:
    """現在時刻が市場稼働時間内かを判定する。"""
    if now.weekday() >= 5: # 土日
        return False
    
    current_time = now.time()
    # 09:00 - 15:30 (昼休みも稼働維持)
    return time(9, 0) <= current_time <= time(15, 30)

def get_seconds_until_next_open(now: datetime) -> float:
    """次の市場開始時刻（平日09:00）までの秒数を計算する。"""
    next_open = now.replace(hour=9, minute=0, second=0, microsecond=0)
    
    if now >= next_open:
        next_open += timedelta(days=1)
    
    while next_open.weekday() >= 5:
        next_open += timedelta(days=1)
        
    return (next_open - now).total_seconds()

def main():
    logger_setup.setup_logging(config.LOG_DIR, log_prefix='realtime', level=config.LOG_LEVEL)
    notifier.start_notifier()
    trader = None

    logger.info("=== StockAutoV3 Realtime Supervisor Started ===")

    try:
        while True:
            now = datetime.now()
            
            if is_market_active(now):
                # --- [A] 市場稼働時間 ---
                if trader is None:
                    logger.info(f"市場オープン ({now.strftime('%H:%M')})。トレーダーを起動します。")
                    trader = RealtimeTrader()
                    trader.start()
                else:
                    time_module.sleep(1)
            else:
                # --- [B] 市場時間外 ---
                if trader is not None:
                    logger.info(f"市場クローズ ({now.strftime('%H:%M')})。トレーダーを停止・データ保存します。")
                    trader.stop()
                    trader = None
                    logger.info("トレーダーの停止が完了しました。")

                wait_seconds = get_seconds_until_next_open(now)
                logger.info(f"次回市場開始まで待機モードに入ります。({wait_seconds / 3600:.1f}時間後)")
                
                sleep_chunk = 60
                while wait_seconds > 0:
                    sleep_time = min(wait_seconds, sleep_chunk)
                    time_module.sleep(sleep_time)
                    wait_seconds -= sleep_time
                    # 途中でCtrl+Cできるようにループを回す
                    # 本格的な運用ではここで再度時刻チェックなどを入れても良いが
                    # 今回は単純なSleepループとする
                    
    except KeyboardInterrupt:
        logger.info("Ctrl+C detected. Shutting down gracefully.")
    except Exception as e:
        logger.critical(f"An unhandled exception occurred in the main thread: {e}", exc_info=True)
    finally:
        if trader:
            logger.info("Performing final cleanup...")
            trader.stop()
        notifier.stop_notifier()
        logger.info("Application has been shut down.")

if __name__ == '__main__':
    main()
```

## 4\. 検証計画

1.  **データ保存テスト**: 市場オープン中に強制的に `Ctrl+C` で停止させ、その時点までの確定足がCSVに保存されるか確認する。
2.  **再起動テスト**: 保存されたCSVが存在する状態で再起動し、ログに "Loaded X bars" と表示され、インジケーターがエラーなく計算されるか確認する。

-----

以上が実装計画書となります。
ご確認いただき、問題がなければ **４，実装（tmp.pyの生成）** へ進みます。